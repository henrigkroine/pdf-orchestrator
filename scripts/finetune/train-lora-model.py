#!/usr/bin/env python3

"""
TEEI Brand LoRA Fine-Tuning Script

Fine-tunes vision-language models (Gemini 2.5 Flash, GPT-4o, Claude Sonnet 4.5)
using LoRA (Low-Rank Adaptation) for TEEI-specific brand compliance.

LoRA Benefits:
- Train with <1% of parameters (500K vs 50B)
- 5000x smaller adapter files (10MB vs 50GB)
- Train on consumer GPU (24GB)
- 2-4 hour training time
- 95% of full fine-tuning performance

Usage:
    python scripts/finetune/train-lora-model.py
    python scripts/finetune/train-lora-model.py --model gemini-2.5-flash
    python scripts/finetune/train-lora-model.py --epochs 5 --batch-size 8
    python scripts/finetune/train-lora-model.py --resume checkpoints/checkpoint-1000
"""

import os
import sys
import json
import argparse
import torch
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple

# Check if running with GPU
HAS_GPU = torch.cuda.is_available()
DEVICE = "cuda" if HAS_GPU else "cpu"

print(f"üñ•Ô∏è  Device: {DEVICE}")
if HAS_GPU:
    print(f"   GPU: {torch.cuda.get_device_name(0)}")
    print(f"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
print()

# Import ML libraries (handle gracefully if not installed)
try:
    from transformers import (
        AutoTokenizer,
        AutoModelForVision2Seq,
        AutoProcessor,
        Trainer,
        TrainingArguments,
        EarlyStoppingCallback
    )
    from peft import (
        LoraConfig,
        get_peft_model,
        PeftModel,
        TaskType
    )
    from datasets import load_dataset
    import wandb
    HAS_DEPENDENCIES = True
except ImportError as e:
    print(f"‚ö†Ô∏è  Missing dependencies: {e}")
    print("   Install with: pip install transformers peft datasets wandb torch")
    HAS_DEPENDENCIES = False

# Project paths
ROOT_DIR = Path(__file__).parent.parent.parent
TRAINING_DATA_DIR = ROOT_DIR / "training-data"
MODELS_DIR = ROOT_DIR / "models"
CHECKPOINTS_DIR = ROOT_DIR / "checkpoints"
LOGS_DIR = ROOT_DIR / "logs" / "training"

# Create directories
for dir_path in [MODELS_DIR, CHECKPOINTS_DIR, LOGS_DIR]:
    dir_path.mkdir(parents=True, exist_ok=True)


class TEEILoRATrainer:
    """
    TEEI-specific LoRA fine-tuning trainer
    """

    def __init__(self, config: Dict):
        self.config = config
        self.model_name = config.get("model_name", "google/gemini-2.5-flash")
        self.output_dir = MODELS_DIR / config.get("output_name", "teei-brand-lora")
        self.checkpoint_dir = CHECKPOINTS_DIR / config.get("output_name", "teei-brand-lora")

        # LoRA configuration
        self.lora_config = LoraConfig(
            r=config.get("lora_r", 16),  # Rank (higher = more capacity, slower)
            lora_alpha=config.get("lora_alpha", 32),  # Scaling factor
            target_modules=config.get("target_modules", ["q_proj", "v_proj", "k_proj", "o_proj"]),
            lora_dropout=config.get("lora_dropout", 0.1),
            bias=config.get("lora_bias", "none"),
            task_type=TaskType.VISION_SEQ_2_SEQ_LM
        )

        # Training configuration
        self.training_args = TrainingArguments(
            output_dir=str(self.checkpoint_dir),
            num_train_epochs=config.get("epochs", 3),
            per_device_train_batch_size=config.get("batch_size", 4),
            per_device_eval_batch_size=config.get("eval_batch_size", 4),
            gradient_accumulation_steps=config.get("gradient_accumulation", 4),
            learning_rate=config.get("learning_rate", 2e-4),
            warmup_steps=config.get("warmup_steps", 100),
            weight_decay=config.get("weight_decay", 0.01),
            logging_steps=config.get("logging_steps", 10),
            eval_steps=config.get("eval_steps", 100),
            save_steps=config.get("save_steps", 500),
            save_total_limit=config.get("save_total_limit", 3),
            evaluation_strategy="steps",
            save_strategy="steps",
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            fp16=HAS_GPU,  # Use mixed precision on GPU
            bf16=False,
            dataloader_num_workers=config.get("num_workers", 4),
            remove_unused_columns=False,
            report_to=["wandb"] if config.get("use_wandb", False) else [],
            run_name=f"teei-lora-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
        )

        self.model = None
        self.tokenizer = None
        self.processor = None
        self.train_dataset = None
        self.val_dataset = None

    def load_base_model(self):
        """Load base vision-language model"""
        print(f"üì• Loading base model: {self.model_name}...\n")

        try:
            # Load tokenizer/processor
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.processor = AutoProcessor.from_pretrained(self.model_name)

            # Load model
            model = AutoModelForVision2Seq.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16 if HAS_GPU else torch.float32,
                device_map="auto" if HAS_GPU else None,
                trust_remote_code=True
            )

            # Print model info
            total_params = sum(p.numel() for p in model.parameters())
            print(f"   ‚úÖ Model loaded")
            print(f"   Total parameters: {total_params:,} ({total_params / 1e9:.1f}B)")
            print()

            return model

        except Exception as e:
            print(f"   ‚ùå Failed to load model: {e}")
            print(f"   Note: Some models require authentication or special access")
            raise

    def apply_lora(self, model):
        """Apply LoRA adapters to model"""
        print(f"üîß Applying LoRA adapters (r={self.lora_config.r})...\n")

        # Add LoRA adapters
        model = get_peft_model(model, self.lora_config)

        # Print trainable parameters
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in model.parameters())
        trainable_percent = 100 * trainable_params / total_params

        print(f"   ‚úÖ LoRA applied")
        print(f"   Trainable parameters: {trainable_params:,} ({trainable_params / 1e6:.1f}M)")
        print(f"   Total parameters: {total_params:,} ({total_params / 1e9:.1f}B)")
        print(f"   Trainable: {trainable_percent:.4f}% üéâ")
        print()

        return model

    def load_datasets(self):
        """Load TEEI training datasets"""
        print("üìö Loading TEEI training datasets...\n")

        train_path = TRAINING_DATA_DIR / "teei-train.jsonl"
        val_path = TRAINING_DATA_DIR / "teei-validation.jsonl"

        if not train_path.exists():
            print(f"   ‚ùå Training dataset not found: {train_path}")
            print(f"   Run: node scripts/finetune/prepare-teei-dataset.js")
            sys.exit(1)

        # Load datasets
        self.train_dataset = load_dataset("json", data_files=str(train_path), split="train")
        self.val_dataset = load_dataset("json", data_files=str(val_path), split="train")

        print(f"   ‚úÖ Training examples: {len(self.train_dataset)}")
        print(f"   ‚úÖ Validation examples: {len(self.val_dataset)}")
        print()

        return self.train_dataset, self.val_dataset

    def preprocess_function(self, examples):
        """Preprocess examples for training"""
        # Extract messages
        messages = examples["messages"]

        # Process images and text
        images = []
        texts = []

        for msg_list in messages:
            for msg in msg_list:
                if msg["role"] == "user":
                    for content in msg["content"]:
                        if content["type"] == "image":
                            images.append(content["image"])
                        elif content["type"] == "text":
                            texts.append(content["text"])
                elif msg["role"] == "assistant":
                    texts.append(msg["content"])

        # Tokenize
        model_inputs = self.processor(
            images=images,
            text=texts,
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=512
        )

        return model_inputs

    def train(self):
        """Execute training"""
        print("üöÄ Starting TEEI LoRA fine-tuning...\n")

        if not HAS_DEPENDENCIES:
            print("‚ùå Cannot train: Missing dependencies")
            print("   Install with: pip install transformers peft datasets wandb torch")
            sys.exit(1)

        # Initialize wandb (optional)
        if self.config.get("use_wandb", False):
            wandb.init(
                project="teei-brand-compliance",
                name=self.training_args.run_name,
                config=self.config
            )

        # Load base model
        base_model = self.load_base_model()

        # Apply LoRA
        self.model = self.apply_lora(base_model)

        # Load datasets
        self.load_datasets()

        # Preprocess datasets
        print("üîÑ Preprocessing datasets...\n")
        train_dataset = self.train_dataset.map(
            self.preprocess_function,
            batched=True,
            remove_columns=self.train_dataset.column_names
        )
        val_dataset = self.val_dataset.map(
            self.preprocess_function,
            batched=True,
            remove_columns=self.val_dataset.column_names
        )
        print("   ‚úÖ Preprocessing complete\n")

        # Initialize trainer
        trainer = Trainer(
            model=self.model,
            args=self.training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
        )

        # Train
        print("üèãÔ∏è  Training...\n")
        training_start = datetime.now()

        train_result = trainer.train(
            resume_from_checkpoint=self.config.get("resume_from_checkpoint")
        )

        training_end = datetime.now()
        training_duration = training_end - training_start

        # Print results
        print(f"\n‚úÖ Training complete!")
        print(f"   Duration: {training_duration}")
        print(f"   Final loss: {train_result.training_loss:.4f}")
        print()

        # Evaluate
        print("üìä Evaluating on validation set...\n")
        eval_result = trainer.evaluate()
        print(f"   Validation loss: {eval_result['eval_loss']:.4f}")
        print()

        # Save model
        print(f"üíæ Saving LoRA adapters...\n")
        self.model.save_pretrained(str(self.output_dir))
        self.tokenizer.save_pretrained(str(self.output_dir))

        # Save training config
        config_path = self.output_dir / "training_config.json"
        with open(config_path, "w") as f:
            json.dump({
                "model_name": self.model_name,
                "lora_config": {
                    "r": self.lora_config.r,
                    "lora_alpha": self.lora_config.lora_alpha,
                    "target_modules": self.lora_config.target_modules,
                    "lora_dropout": self.lora_config.lora_dropout
                },
                "training_args": {
                    "epochs": self.training_args.num_train_epochs,
                    "batch_size": self.training_args.per_device_train_batch_size,
                    "learning_rate": self.training_args.learning_rate
                },
                "training_results": {
                    "training_loss": train_result.training_loss,
                    "validation_loss": eval_result["eval_loss"],
                    "duration_seconds": training_duration.total_seconds()
                },
                "trained_at": training_end.isoformat()
            }, f, indent=2)

        print(f"   ‚úÖ Model saved to: {self.output_dir}")
        print(f"   ‚úÖ Config saved to: {config_path}")
        print()

        # Calculate file size
        adapter_size = sum(
            f.stat().st_size for f in self.output_dir.rglob("*") if f.is_file()
        ) / (1024 * 1024)  # MB

        print(f"üì¶ LoRA Adapter Statistics:")
        print(f"   Adapter size: {adapter_size:.1f} MB")
        print(f"   Base model size: ~50 GB (not saved, only adapters!)")
        print(f"   Space savings: {50000 / adapter_size:.0f}x smaller üéâ")
        print()

        print("üéâ TEEI brand model ready!")
        print("   Use with: node scripts/lib/teei-custom-model.js")
        print()

        return {
            "model_path": str(self.output_dir),
            "training_loss": train_result.training_loss,
            "validation_loss": eval_result["eval_loss"],
            "adapter_size_mb": adapter_size,
            "training_duration": str(training_duration)
        }


class TEEIModelEvaluator:
    """
    Evaluate trained LoRA model
    """

    def __init__(self, model_path: Path):
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self.processor = None

    def load_model(self):
        """Load fine-tuned model"""
        print(f"üì• Loading fine-tuned TEEI model from {self.model_path}...\n")

        # Load config
        config_path = self.model_path / "training_config.json"
        with open(config_path) as f:
            config = json.load(f)

        base_model_name = config["model_name"]

        # Load base model
        base_model = AutoModelForVision2Seq.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16 if HAS_GPU else torch.float32,
            device_map="auto" if HAS_GPU else None
        )

        # Load LoRA adapters
        self.model = PeftModel.from_pretrained(base_model, str(self.model_path))
        self.tokenizer = AutoTokenizer.from_pretrained(str(self.model_path))
        self.processor = AutoProcessor.from_pretrained(base_model_name)

        print("   ‚úÖ Model loaded with LoRA adapters\n")

    def validate_document(self, image_path: str, prompt: str) -> Dict:
        """
        Validate a TEEI document using fine-tuned model
        """
        if self.model is None:
            self.load_model()

        # Process input
        inputs = self.processor(
            images=[image_path],
            text=[prompt],
            return_tensors="pt"
        ).to(DEVICE)

        # Generate validation
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=1024,
                temperature=0.3,
                do_sample=True
            )

        # Decode response
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Parse JSON response
        try:
            result = json.loads(response)
        except json.JSONDecodeError:
            result = {"raw_response": response}

        return result

    def benchmark(self, test_dataset_path: Path):
        """
        Benchmark model on test dataset
        """
        print("üìä Benchmarking TEEI model...\n")

        # Load test examples
        with open(test_dataset_path) as f:
            test_examples = [json.loads(line) for line in f]

        results = []
        correct = 0

        for i, example in enumerate(test_examples):
            print(f"   Testing {i+1}/{len(test_examples)}...", end="\r")

            # Get expected grade
            expected = example["messages"][1]["content"]
            expected_data = json.loads(expected)
            expected_grade = expected_data["grade"]

            # Validate with model
            user_message = example["messages"][0]
            image_path = user_message["content"][0]["image"]
            prompt = user_message["content"][1]["text"]

            result = self.validate_document(image_path, prompt)
            predicted_grade = result.get("grade", "Unknown")

            # Check accuracy
            is_correct = predicted_grade == expected_grade
            if is_correct:
                correct += 1

            results.append({
                "expected": expected_grade,
                "predicted": predicted_grade,
                "correct": is_correct
            })

        accuracy = correct / len(test_examples)

        print(f"\n   ‚úÖ Benchmark complete")
        print(f"   Accuracy: {accuracy:.1%} ({correct}/{len(test_examples)})")
        print()

        return {
            "accuracy": accuracy,
            "correct": correct,
            "total": len(test_examples),
            "results": results
        }


def main():
    parser = argparse.ArgumentParser(description="Train TEEI LoRA model")
    parser.add_argument("--model", default="google/gemini-2.5-flash", help="Base model name")
    parser.add_argument("--output", default="teei-brand-lora", help="Output directory name")
    parser.add_argument("--epochs", type=int, default=3, help="Number of training epochs")
    parser.add_argument("--batch-size", type=int, default=4, help="Training batch size")
    parser.add_argument("--learning-rate", type=float, default=2e-4, help="Learning rate")
    parser.add_argument("--lora-r", type=int, default=16, help="LoRA rank")
    parser.add_argument("--lora-alpha", type=int, default=32, help="LoRA alpha")
    parser.add_argument("--resume", help="Resume from checkpoint")
    parser.add_argument("--use-wandb", action="store_true", help="Enable Weights & Biases logging")
    parser.add_argument("--evaluate", help="Evaluate existing model (provide model path)")
    parser.add_argument("--benchmark", help="Benchmark model on test set (provide test file)")

    args = parser.parse_args()

    # Evaluation mode
    if args.evaluate:
        evaluator = TEEIModelEvaluator(Path(args.evaluate))
        if args.benchmark:
            evaluator.benchmark(Path(args.benchmark))
        else:
            print("‚ÑπÔ∏è  Use --benchmark <test_file> to run benchmarks")
        return

    # Training mode
    config = {
        "model_name": args.model,
        "output_name": args.output,
        "epochs": args.epochs,
        "batch_size": args.batch_size,
        "learning_rate": args.learning_rate,
        "lora_r": args.lora_r,
        "lora_alpha": args.lora_alpha,
        "resume_from_checkpoint": args.resume,
        "use_wandb": args.use_wandb
    }

    trainer = TEEILoRATrainer(config)
    result = trainer.train()

    print("‚ú® Training complete!")
    print(f"   Model: {result['model_path']}")
    print(f"   Training loss: {result['training_loss']:.4f}")
    print(f"   Validation loss: {result['validation_loss']:.4f}")
    print(f"   Adapter size: {result['adapter_size_mb']:.1f} MB")
    print(f"   Duration: {result['training_duration']}")
    print()


if __name__ == "__main__":
    main()
